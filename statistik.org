* TODO Allgemeines
** DONE data access
vek[vek<0]
mat[ ,1]
mat[1, ]
** DONE subset
subset(x = data vector, subset = logical expression, )
e.g. subset(potato, Sorte==1|Sorte==3)
** DONE rep
rep(x = data, times = numeric value)
** DONE seq
seq.int(from, to, by, ) # length.out
** DONE matrix
# sigle type for whole matrix
matrix(x = data, ncol, nrow, byrow=FALSE)
** DONE data.frame
data.frame(... = spalten, row.names=NULL)
    row.names: NULL | string = column to be used as row names | vector of names
** DONE read.table
   read.table("filename", header = FALSE, sep = '')
** DONE write.table
write.table(potato, "filename", sep = '\t', dec = ".")
** TODO levels, relevel
BRASSICA$group <- ordered(BRASSICA$group, c("control", "ancy"))
** TODO naming col/row
** convert to/from flatfiles
colnames(matrix) <- c(names)
* DONE Deskriptive Statistik
** DONE Mittelwert
mean(datenfeld x)
** DONE Standardabweichung
sd(zahlenfeld x)
** DONE Varianz
var(x)
** DONE Min,Max,Median, Quantile
min(numeric objects)
max(numeric objects)
median(...)
quantile(x=numeric vector, p=propability vector) # p == probs == wahrscheinlichkeiten. 0.25 fürs erste und 0.75 fürs 4. quartil
example: quantile(1:10, p=c(0.25,0.75))
** DONE tapply
tapply(Vek X, INDEX = index zum gruppieren, FUN = NULL)
tapply(potato$Ertrag, INDEX=potato$Sorte, FUN=min)
** DONE summary
summary(object)

* TODO Grafiken in R
** DONE formula
values y ~ grouping factor
** TODO formula = ~ x + y
** DONE hist
hist(x=value vector, breaks=c(bereiche der balken))
** TODO boxplot
boxplot(formula, data = NULL)
*** TODO param subset

** TODO barplot
** DONE scatterplot
plot(x, y)
** DONE plot
plot(x = 2d data)
plot(x, y)
** DONE fancy graphics
arguments:
- main="titel"
- col = c(colorstrings)
- ylab = "Achsenbeschriftung"
- xlab = "Achsenbeschriftung"
** TODO export graphics
nach dem öffnen einer grafik:
dev.print(pdf, "filename.pdf")
* DONE Zweistichprobentests
| Test         | VH | NV |
|--------------+----+----|
| t-test       | x  | x  |
| t-Welch-test |    | x  |
| Wilcoxon     | x  |    |

Der t-Test ist der Hypothesentest der t-Verteilung.
Er kann verwendet werden, um zu bestimmen, ob zwei Stichproben sich statistisch signifikant unterscheiden. Meistens wird der t-Test dort eingesetzt, wo die Testgröße normalverteilt wäre, wenn der Skalierungsparameter (der Parameter, der die Streuung definiert — bei einer normalverteilten Zufallsvariable die Standardabweichung) bekannt wäre.
Ist der Skalierungsparameter unbekannt, wird er durch eine Schätzung aus dem Datensatz ersetzt.

Hypothesen:
Wir wollen die Alternativhypothese beweisen indem wir die Nullhypothese widerlegen. Die Nullhypothese muss immer (je nachdem, was man zeigen will) die Gleichheit der Vergleichsmerkmale enthalten.
Es gibt 3 Möglichkeiten:
alternative = c("less", "greater", "two.sided")
# H1: Die gesunden Mauese waren schneller.
# H0: Die gesunden Mauese waren gleich schnell oder langsamer.
** 
** DONE t-test
Vorrausetzungen: VH, NV
!!! var.equal=TRUE für den normalen t-test
t.test(x, y=NULL, alternative = c("less", "greater", "two.sided"), paired=FALSE, var.equal=FALSE, conf.level=0.95)
** DONE t-welch-test
Vorrausetzungen: NV
wie t.test, nur mit var.equal=FALSE
t.test(x, y=NULL, alternative = c("less", "greater", "two.sided"), paired=FALSE, var.equal=FALSE, conf.level=0.95)
** DONE paired test
Wenn zwei Messungen ‘verbunden’ (= ‘gepaart’) sind (z.B. ein Patient vor und nach einer Behandlung) nimmt man den gepaarten Test.
** TODO wilcoxon-test
Vorrausetzungen: VH
wilcox.test(x, y = NULL, alternative = c("two.sided", "less", "greater"), paired = FALSE, exact = NULL, correct=FALSE, conf.level = 0.95)
wilcox.test(formula=content~type, data=hefe,alternative="less",conf.level=0.95,dist="exact")
TODO: exact, dist, correct
# what the hell ist dist="exact"
* DONE chi-quadrat-test
** DONE Anpassungstest
überprüfen, ob daten einer bestimmten (gegebenen) verteilung folgen
H0 : F_Versuch (x) = F_Mendel (x)
H1 : F_Versuch (x) != F_Mendel (x)
chisq.test(c(60,16,20,4), c(9,3,3,1))
** DONE Homogenitätstest
Testen ob zwei Datensätze der selben Verteilung folgen

H0 : πüberexpr. (x) = πnormal (x)
H1 : πüberexpr. (x) = πnormal (x)
chisq.test(matrix(c(12,9,14,7), ncol=2), correct = FALSE)

# X-squared = 0.4038, df = 1, p-value = 0.5251
# Je nachdem, von welchem alpha-Fehler man ausgeht, ist p signifikant oder nicht. Nimmt man alpha = 5% = 0,05 an, so ist p nicht signifikant und somit wird die Nullhypothese angenommen.
* DONE Korrelationsanalyse
prüft, ob zwischen datensätzen eine korelation vorliegt.
** DONE Korelationskoeffizient
Der Korelationskoeffizient gibt an, wie stark daten miteinander korelieren, also z.B. wie stark die steuung des linearen zusammenhangs ist.
cor(x, y=NULL, method = "spearman"|"pearson")
** DONE analyse
cor.test(formula = ~Rumpf+Widerrist, data = cattle, method = "spearman", alternative = "greater”)
#Da p-Wert kleiner als 0.1, ist Korrelation signifikant.
#Korrelationswert von 0.58 bedeutet mäßig starke Korrelation zwischen Rumpfhöhe und Widerrist.
** DONE Pearson
wenn alle Datensätze NV ist
** DONE Spearman
wenn einer der Datensätze nicht NV ist
* DONE Residuen
Rediduen sind die Abweichungen vom Modell (also etwa einer linearen Regression)
Lassen sie ein Muster erkennen, so ist vermutlich das Modell nicht geeignet. Wir müssen dann VarianzINhomogenität annehmen.
http://www.statistics4u.info/fundstat_germ/img/hl_residuals.png
* Regressionsanalyse
* Anova
* Multiple Mittelwerttests
* Varianzhomogenität
** levene-test
* Normalverteilung
** testen
** nomalverteilung generieren
* TODO Hypothesen und Ergebnisse der Test interpretieren
* DONE Freiheitsgrade
in R (ihm) oft mit df angegeben.

f = n - u
n .. Anzahl der Daten
u .. Schätzbare Parameter

Also wenn man etwa nur den Mittelwert (u=1) und n=10 Werte hat, dann ist der Freiheitsgrad f = 9, weil man mithilfe von 9 Werten und dem Mittelwert den 10. Berechnen kann.
Die Freiheitsgrade werden bei der Schätzung von Varianzen benötigt. (und spielen bei manchem Hypothesentests eine Rolle)
